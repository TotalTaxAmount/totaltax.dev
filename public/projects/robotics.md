---
title: FRC team 2036
thumbnail: 2036.jpg
---
I am on a First Robotics team called **The Black Knights**. FIRST Robotics Competition is robotics competition founded in 1989 to teach kids STEM. There are multiple different competition such as FIRST Lego League (FLL), FIRST Tech Challenge (FTC), and FIRST Robotics Competition (FRC), 2036 is an FRC team, FRC games have 2 main phases, the autonomous phase, where the robot is fully autonomous with no human input, and the Tele-operated phase, where the robot is controlled by 1-2 drivers. Team 2036 was founded in 2007 by 2 Fairview Highschool students, starting out in a classroom at Fairview the team eventually moved to the [**Solid State Depot**](https://www.ssdmakerspace.org/). In the early years of the team there where very few students and mentors on team, however as the team aged it grew in size to now having around 20-25 active members. I joined the team in 2022 after the main season had finished and spent the summer figuring out how the team worked and how programming the robot was done.

2023:
---
 When my first season began I was on the Programming sub-team. In that year we used a programming language called [Kotlin](https://kotlinlang.org/), Kotlin is intended to be a replacement for the popular language Java. Having previous experience in Java it was easy for me to pickup Kotlin. At the time we where using a piece of camera hardware called LimeLight. LimeLight is designed specifically for FIRST robotics.
![Limelight](/projects/images/robotics/limelight.png)*The limelight 3 (https://limelightvision.io/cdn/shop/files/LL3A2_1080_1080x.png?v=1673323742)*

The LimeLight contains a single camera, 6 green LEDs, and a small single board computer. Once it is connected to the robots network via ethernet I could use its web-ui to configure it to our needs, more specifically to track small QR code like images called AprilTags that contain an integer, the competition field has AprilTags positioned in specific locations that all have there own ID (1-16). Using these it is possible for the LimeLight to determine a pretty good estimate of the robot's location on the field. I could then use this information, combined with the wheels rotation data to get a pretty accurate way to get the robots position on the field. This could then be used to align the robot to specific locations for scoring items or picking them up from a know location. While this was very powerful and had good potential, it was not reliable enough to be used in competition. If the robot was positioned in such a way that the AprilTag was on the very edge of the LimeLight's FOV it would completely freak out and would give coordinates that where very off. We made good progress on vision but didn't get to use it during competition. 
![2023 Competition](/projects/images/robotics/2023comp.png)*Our robot during the 2023 Colorado Regional, center of image*

2024:
---
My second year on the team I was the Mechanism lead for programming, this meant that I was in charge of writing the subsystems (responsible for controlling motors and other basic hardware functions) and the commands (uses 1 or more subsystems combined to complete a task like driving or picking something up, they are bound to buttons on the controller). This year I also helped lead an effort to switch over to Java from Kotlin because most people on the team already had experience in Java and there was no good reason to use Kotlin, I also pushed to have good documentation on our code, in 2023, there was little to no documentation in our codebase, making it very hard for people not unfamiliar with parts of the code to figure out what was going on. Once the design of the robot was finalized I began writing code for all of the different mechanisms, this was a pretty straight forward task that mostly involved spinning motors to a certain speed or moving something to a specific location. To accomplish both of these I used a algorithm called Proportional–integral–derivative controller or PID. PIDs where very important for controlling the motors, they use a feedback loop to reach a target. 
![PID](/projects/images/robotics/PID.png)*Diagram of how a pid works (https://www.azom.com/article.aspx?ArticleID=22851)*

After finishing the code before there was enough robot to test it, I moved back to vision and robot odometry. I was able to get back to what I had last year pretty quick and was able to better integrate that vision data into the odometry system. Using this better positional data we started integrating into the autonomous phase of our robot in hopes to allow it to correct for positioning errors, while this system appeared to be working as intended we had a very peculiar issue with the PIDs used to control the motors during autonomous. They would overshoot the target speed, then way over correct and repeat. No matter the values we used to tune the PIDs they just wound not work correctly, so for the first competition we had to disable vision. However during the second competition after rereading all of the code that the autonomous system would call, I realized someone had made an error in documentation, on the main drive function in the subsystem for the driving system, it was documented that the function took in meters/second for forward and sideways, and radians/second for rotation. However after reading the code I realized the function was multiplying the sideways, forward, and sideways speeds by the maximum speed of the robot, effectively making it a percentage rather than m/s. So if the PID tried to instruct the robot to go forward 3.5 meters per second, it would attempt to go 3500% of the max speed. After fixing this issue the autonomous mode work much better and we where able to use vision during the competition.

2025:
---
**...**
